{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc53fa3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check if required directories exist\n",
    "for dir_name in ['data', 'checkpoints', 'outputs', 'src']:\n",
    "    dir_path = Path(dir_name)\n",
    "    if dir_path.exists():\n",
    "        print(f\"✓ {dir_name}/ exists\")\n",
    "    else:\n",
    "        print(f\"✗ {dir_name}/ not found\")\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"  Created {dir_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d98147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의존성 설치 (필요시 실행)\n",
    "INSTALL_DEPENDENCIES = False  # True로 변경하면 requirements.txt 설치\n",
    "\n",
    "if INSTALL_DEPENDENCIES:\n",
    "    print(\"Installing dependencies from requirements.txt...\")\n",
    "    result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], \n",
    "                          capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✓ Dependencies installed successfully\")\n",
    "    else:\n",
    "        print(\"✗ Installation failed\")\n",
    "        print(result.stderr)\n",
    "else:\n",
    "    print(\"⊘ Dependency installation skipped\")\n",
    "    print(\"  (Set INSTALL_DEPENDENCIES = True to install)\")\n",
    "    \n",
    "# Check key packages\n",
    "print(\"\\nChecking key packages...\")\n",
    "required_packages = ['torch', 'transformers', 'sklearn', 'tqdm', 'numpy', 'matplotlib']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"  ✓ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ✗ {package} (missing)\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\n⚠ Missing packages: {', '.join(missing_packages)}\")\n",
    "    print(\"  Set INSTALL_DEPENDENCIES = True and re-run this cell\")\n",
    "else:\n",
    "    print(\"\\n✓ All required packages are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f2526",
   "metadata": {},
   "source": [
    "## 의존성 설치 (선택사항)\n",
    "\n",
    "첫 실행 시 필요한 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38265e",
   "metadata": {},
   "source": [
    "## 전체 실행 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행할 단계 선택 (True/False)\n",
    "RUN_CONFIG = {\n",
    "    'preprocess': True,      # 1. 데이터 전처리\n",
    "    'train_baseline': True,  # 2. Baseline 모델 학습\n",
    "    'generate_text': True,   # 3. 대화 생성\n",
    "    'train_multimodal': True,# 4. Multimodal 모델 학습\n",
    "    'evaluate': True,        # 5. 평가\n",
    "    \n",
    "    # 평가 설정\n",
    "    'n_eval_games': 1000,    # 평가할 게임 수\n",
    "}\n",
    "\n",
    "print(\"실행 설정:\")\n",
    "for key, value in RUN_CONFIG.items():\n",
    "    print(f\"  {key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9a8a6",
   "metadata": {},
   "source": [
    "## Step 1: 데이터 전처리\n",
    "\n",
    "Pluribus 데이터셋을 다운로드하고 377차원 feature로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98edfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CONFIG['preprocess']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 1: DATA PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = subprocess.run(['python', '1_preprocess_data.py'], capture_output=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✓ Step 1 completed successfully\")\n",
    "    else:\n",
    "        print(\"\\n✗ Step 1 failed\")\n",
    "        print(\"Stopping pipeline...\")\n",
    "        raise RuntimeError(\"Preprocessing failed\")\n",
    "else:\n",
    "    print(\"⊘ Step 1 skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45ff85",
   "metadata": {},
   "source": [
    "## Step 2: Baseline 모델 학습\n",
    "\n",
    "게임 상태만 사용하는 MLP 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37874870",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CONFIG['train_baseline']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 2: TRAIN BASELINE MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = subprocess.run(['python', '2_train_baseline.py'], capture_output=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✓ Step 2 completed successfully\")\n",
    "    else:\n",
    "        print(\"\\n✗ Step 2 failed\")\n",
    "        print(\"Stopping pipeline...\")\n",
    "        raise RuntimeError(\"Baseline training failed\")\n",
    "else:\n",
    "    print(\"⊘ Step 2 skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e209f43",
   "metadata": {},
   "source": [
    "## Step 3: 대화 생성\n",
    "\n",
    "LLM을 사용하여 각 게임 상태에 대한 대화를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b94a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CONFIG['generate_text']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 3: GENERATE DIALOGUES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = subprocess.run(['python', '3_generate_dialogues.py'], capture_output=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✓ Step 3 completed successfully\")\n",
    "    else:\n",
    "        print(\"\\n✗ Step 3 failed\")\n",
    "        print(\"Stopping pipeline...\")\n",
    "        raise RuntimeError(\"Dialogue generation failed\")\n",
    "else:\n",
    "    print(\"⊘ Step 3 skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b82fb",
   "metadata": {},
   "source": [
    "## Step 4: Multimodal 모델 학습\n",
    "\n",
    "게임 상태와 대화를 결합한 multimodal 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04fed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CONFIG['train_multimodal']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 4: TRAIN MULTIMODAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    result = subprocess.run(['python', '4_train_multimodal.py'], capture_output=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✓ Step 4 completed successfully\")\n",
    "    else:\n",
    "        print(\"\\n✗ Step 4 failed\")\n",
    "        print(\"Stopping pipeline...\")\n",
    "        raise RuntimeError(\"Multimodal training failed\")\n",
    "else:\n",
    "    print(\"⊘ Step 4 skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49e7a0",
   "metadata": {},
   "source": [
    "## Step 5: Rule-based Agent 평가\n",
    "\n",
    "학습된 모델을 rule-based agent와 대결시켜 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a0ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_CONFIG['evaluate']:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STEP 5: EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    n_games = RUN_CONFIG['n_eval_games']\n",
    "    \n",
    "    # Evaluate baseline\n",
    "    print(\"\\n--- Evaluating Baseline Model ---\")\n",
    "    result_baseline = subprocess.run([\n",
    "        'python', '5_evaluate_vs_rule_based.py',\n",
    "        '--model_type', 'baseline',\n",
    "        '--model_path', 'checkpoints/baseline_best.pt',\n",
    "        '--n_games', str(n_games),\n",
    "        '--output_dir', 'outputs'\n",
    "    ], capture_output=False)\n",
    "    \n",
    "    if result_baseline.returncode != 0:\n",
    "        print(\"\\n✗ Baseline evaluation failed\")\n",
    "    \n",
    "    # Evaluate multimodal\n",
    "    print(\"\\n--- Evaluating Multimodal Model ---\")\n",
    "    result_multimodal = subprocess.run([\n",
    "        'python', '5_evaluate_vs_rule_based.py',\n",
    "        '--model_type', 'multimodal',\n",
    "        '--model_path', 'checkpoints/multimodal_best.pt',\n",
    "        '--n_games', str(n_games),\n",
    "        '--output_dir', 'outputs'\n",
    "    ], capture_output=False)\n",
    "    \n",
    "    if result_multimodal.returncode != 0:\n",
    "        print(\"\\n✗ Multimodal evaluation failed\")\n",
    "    \n",
    "    if result_baseline.returncode == 0 and result_multimodal.returncode == 0:\n",
    "        print(\"\\n✓ Step 5 completed successfully\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Step 5 completed with errors\")\n",
    "else:\n",
    "    print(\"⊘ Step 5 skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d93604",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ac3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check generated files\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_dir = Path('checkpoints')\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = list(checkpoint_dir.glob('*.pt'))\n",
    "    print(f\"\\nCheckpoints ({len(checkpoints)}):\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  - {ckpt.name:40s} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Outputs\n",
    "output_dir = Path('outputs')\n",
    "if output_dir.exists():\n",
    "    outputs = list(output_dir.glob('*'))\n",
    "    print(f\"\\nOutputs ({len(outputs)}):\")\n",
    "    for out in sorted(outputs):\n",
    "        if out.is_file():\n",
    "            size = out.stat().st_size\n",
    "            if size < 1024:\n",
    "                size_str = f\"{size} B\"\n",
    "            elif size < 1024*1024:\n",
    "                size_str = f\"{size/1024:.1f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size/(1024*1024):.1f} MB\"\n",
    "            print(f\"  - {out.name:40s} ({size_str})\")\n",
    "\n",
    "# Load and display evaluation results\n",
    "baseline_file = output_dir / 'baseline_vs_rule_based.json'\n",
    "multimodal_file = output_dir / 'multimodal_vs_rule_based.json'\n",
    "\n",
    "if baseline_file.exists() and multimodal_file.exists():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with open(baseline_file, 'r') as f:\n",
    "        baseline_results = json.load(f)\n",
    "    \n",
    "    with open(multimodal_file, 'r') as f:\n",
    "        multimodal_results = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{'Metric':<30s} {'Baseline':<15s} {'Multimodal':<15s}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Agreement Rate':<30s} {baseline_results['agreement_rate']:>14.2f}% {multimodal_results['agreement_rate']:>14.2f}%\")\n",
    "    print(f\"{'Games Played':<30s} {baseline_results['n_games']:>14d} {multimodal_results['n_games']:>14d}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ PIPELINE COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n⚠ Evaluation results not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df21a5",
   "metadata": {},
   "source": [
    "## 완료!\n",
    "\n",
    "전체 파이프라인이 실행되었습니다.\n",
    "\n",
    "### 생성된 파일들\n",
    "- `data/processed/` - 전처리된 데이터\n",
    "- `data/text/` - 생성된 대화\n",
    "- `checkpoints/` - 학습된 모델\n",
    "- `outputs/` - 평가 결과 및 시각화\n",
    "\n",
    "### 다음 단계\n",
    "- [evaluate_models.ipynb](evaluate_models.ipynb)에서 상세한 결과 분석 및 시각화\n",
    "- 개별 스크립트를 직접 실행하여 파라미터 조정"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
